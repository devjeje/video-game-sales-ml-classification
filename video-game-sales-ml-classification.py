# -*- coding: utf-8 -*-
"""FP BDPA Lanjut.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tKi0zpbv0AqlvqdW1Bsde742MNQw7KSZ

# Identitas Kelompok
Anggota:
* Eunique Lydia Stephany (22.11.4545)
* Bima Pratama (22.11.4547)
* Guruh Jodi Saputra (22.11.4569)
* Zefantio (22.11.4580)
* Thomas Wendra Atmadja (22.11.4586)

## 1a. Bidang yang diambil sebagai topik oleh kelompok kami adalah

**(Tolong Parafrase)**
Video Game alasanny karena data ini.... menarik(?), cukup berkembang(?)

## 1b. Proses mendapatkan data

**(tolong parafrase)**
cari di kaggle terus hubungi API ke collab wkwkwk

## 1c. Preprocessing
Pre-processing meliputi banyak hal yaitu:

### Import Dataset
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("willianoliveiragibin/video-game-sales-analyze")

print("Path to dataset files:", path)

import os

# Path dataset di Kaggle
dataset_path = "/root/.cache/kagglehub/datasets/willianoliveiragibin/video-game-sales-analyze/versions/1"

# List file dalam direktori
print(os.listdir(dataset_path))

from pyspark.sql import SparkSession

# Membuat SparkSession
spark = SparkSession.builder.appName("KaggleDataAnalysis").getOrCreate()

# Mengimpor data CSV ke PySpark DataFrame
data_path = "/root/.cache/kagglehub/datasets/willianoliveiragibin/video-game-sales-analyze/versions/1/vgsales new.csv"
df = spark.read.csv(data_path, header=True, inferSchema=True)

# Tampilkan 5 baris pertama
df.show(5)

"""### Data Cleaning

Pada tahap ini dataset akan dicek kualitasnya mulai dari ada tidaknya nilai null, jumlah unique value, hingga penghalusan outlier.
"""

from pyspark.sql.functions import col

# Mengubah kolom 'year' menjadi integer
df = df.withColumn('Year', col('Year').cast('int'))

# Menampilkan beberapa baris untuk memverifikasi perubahan
df.show(5)

df.printSchema()

from pyspark.sql import functions as F

# Menghitung jumlah nilai null di setiap kolom
null_counts = df.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in df.columns])

# Menampilkan hasil jumlah nilai null per kolom
null_counts.show()

# Menghapus nilai null pada setiap kolom
df = df.dropna()
# Menghitung jumlah nilai null di setiap kolom
null_counts = df.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in df.columns])

# Menampilkan hasil jumlah nilai null per kolom
null_counts.show()

numerical_df = df.select( "Year", "NA_Sales", "EU_Sales", "JP_Sales", "Other_Sales", "Global_Sales")
numerical_df.show()

categorical_df = df.select("Name", "Platform", "Genre", "Publisher")
categorical_df.show()

"""#### Normalisasi Numerik"""

from pyspark.sql import functions as F

# Ambil daftar kolom numerik
numerical_column = numerical_df.columns
# Buat agregasi dinamis untuk setiap kolom
agg_exprs = []
for col in numerical_column:
    agg_exprs.append(F.min(col).alias(f"Min_{col}"))
    agg_exprs.append(F.max(col).alias(f"Max_{col}"))

# Cek nilai min dan max menggunakan agg
result = numerical_df.agg(*agg_exprs).collect()

# Tampilkan hasil
for r in result:
    for col in numerical_column:
        print(f"Min {col}: {r[f'Min_{col}']}, Max {col}: {r[f'Max_{col}']}")

col1 = numerical_df.columns
print(col1)

data_values = numerical_df.rdd.map(lambda row: row.asDict().values()).collect()
# Konversi data_values menjadi list of lists
data_values_list = [list(values) for values in data_values]
for values in data_values_list[:5]:
    print(list(values))

from pyspark.sql.functions import col, percentile_approx

data_numeric = spark.createDataFrame(data_values_list, col1)
data_numeric.show(5)

#Penghalusan outlier
# Iterasi untuk semua kolom numerik
from pyspark.sql.functions import col, when
for column in col1:
    q1 = data_numeric.approxQuantile(column, [0.25], 0.0)[0]
    q3 = data_numeric.approxQuantile(column, [0.75], 0.0)[0]
    iqr = q3 - q1
    print(f"Column: {column}, Q1: {q1}, Q3: {q3}, IQR: {iqr}")

    # Tentukan batas bawah dan atas
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    df_numeric_cleaned = data_numeric.withColumn(
        f"{column}_smoothed",
        when(col(column) < lower_bound, lower_bound)
        .when(col(column) > upper_bound, upper_bound)
        .otherwise(col(column))
    )

df_numeric_cleaned.show(5)

# Cek kembali
# Ambil daftar kolom numerik
numerical_column = df_numeric_cleaned.columns

# Buat agregasi dinamis untuk setiap kolom
agg_exprs = []
for col in df_numeric_cleaned:
    agg_exprs.append(F.min(col).alias(f"Min_{col}"))
    agg_exprs.append(F.max(col).alias(f"Max_{col}"))

# Cek nilai min dan max menggunakan agg
result = df_numeric_cleaned.agg(*agg_exprs).collect()

# Tampilkan hasil
for r in result:
    for col in df_numeric_cleaned:
        print(f"Min {col}: {r[f'Min_{col}']}, Max {col}: {r[f'Max_{col}']}")

# Hapus kolom Global_Sales dan ganti nama kolom Global_Sales_smoothed menjadi Global_Sales
df_numeric_cleaned = df_numeric_cleaned.drop("Global_Sales").withColumnRenamed("Global_Sales_smoothed", "Global_Sales")

# Tampilkan hasil DataFrame yang telah diperbarui
df_numeric_cleaned.show()

"""#### Normalisasi Categorical"""

col2 = categorical_df.columns
col2.remove('Name')
print(col2)

raw_cat = categorical_df.drop('Name')
data_values2 = raw_cat.rdd.map(lambda row: row.asDict().values()).collect()
# Konversi data_values menjadi list of lists
data_values2_list = [list(values) for values in data_values2]
for values in data_values2_list[:5]:
    print(list(values))

categorical_data = spark.createDataFrame(data_values2_list, col2)
categorical_data.show(5)

from pyspark.sql.functions import lower

# Iterasi melalui setiap kolom dalam DataFrame
for column in categorical_data.columns:
    # Mengubah semua nilai dalam kolom menjadi huruf kecil
    categorical_data = categorical_data.withColumn(column, lower(categorical_data[column]))

# Menampilkan hasil setelah preprocessing
categorical_df.show(5)

for column in categorical_data.columns:
    unique_count = categorical_data.select(column).distinct().count()
    print(f"Jumlah nilai unik di kolom '{column}': {unique_count}")

# Iterasi melalui setiap kolom dalam categorical_df
for column in categorical_data.columns:
    categorical_data.select(column).distinct().show()

"""## 1d. Exploratory Data Analysis"""

# Import library yang dibutuhkan
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Mengonversi DataFrame PySpark ke Pandas DataFrame
pandas_numeric_df = df_numeric_cleaned.toPandas()

pandas_numeric_df.head()

# Mengonversi DataFrame PySpark ke Pandas DataFrame
pandas_categorical_df = categorical_data.toPandas()

pandas_categorical_df.head()

# @title Pie Chart Genre
genre_counts = pandas_categorical_df['Genre'].value_counts()

# Membuat pie chart
plt.figure(figsize=(8, 6))
genre_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140, legend=False)
plt.title('Pie Chart Genre')
plt.axis('equal')  # Mengatur sumbu menjadi lingkaran
plt.show()

# @title Bar Chart Numerik
# Mengagregasi data berdasarkan tahun
df_yearly_sales = pandas_numeric_df.groupby('Year')[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].sum()

# Membuat bar chart untuk setiap wilayah
df_yearly_sales.plot(kind='bar', figsize=(8, 6))

# Menambahkan judul dan label
plt.title('Penjualan Per Wilayah Berdasarkan Tahun', fontsize=16)
plt.xlabel('Tahun', fontsize=12)
plt.ylabel('Total Penjualan', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()

# Menampilkan plot
plt.show()

#Gabungin numerik dan kategorikal
df_combined = pd.concat([pandas_numeric_df, pandas_categorical_df], axis=1)
df_combined.head()

df_combined.info()

# Menggunakan LabelEncoder dari scikit-learn
from sklearn.preprocessing import LabelEncoder

# Label Encoding untuk Genre
le_genre = LabelEncoder()
df_combined['Genre_encoded'] = le_genre.fit_transform(df_combined['Genre'])

# Label Encoding untuk Publisher
le_publisher = LabelEncoder()
df_combined['Publisher_encoded'] = le_publisher.fit_transform(df_combined['Publisher'])

# Label Encoding untuk Platform
le_platform = LabelEncoder()
df_combined['Platform_encoded'] = le_platform.fit_transform(df_combined['Platform'])

print(df_combined)

# Drop kolom Genre, Platform, dan Publisher
df_combined = df_combined.drop(columns=['Genre', 'Platform', 'Publisher'])

# Ganti nama kolom yang telah diencode
df_combined = df_combined.rename(columns={
    'Genre_encoded': 'Genre',
    'Publisher_encoded': 'Publisher',
    'Platform_encoded': 'Platform'
})

# Tampilkan hasil
df_combined.head()

corr_matrix = df_combined.corr()

# Membuat heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

boxplot_data = pandas_numeric_df.drop(columns=['Year'])
sns.boxplot(data=boxplot_data)
plt.title('Box Plot')
plt.show()

"""## **Persiapan**"""

# Gunakan 'data' sebagai dataframe dan pakai ML dari pyspark jika butuh numerik
data_num = spark.createDataFrame(df_combined)
data_num.show(5)

# Gunakan data2 sebagai data frame yang tipe datanya tidak diubah numerik semua
raw= pd.concat([pandas_numeric_df, pandas_categorical_df], axis=1)
data2 = spark.createDataFrame(raw)
data2.show(5)

data_num.printSchema()
data2.printSchema()

# menemukan fitur-fitur (variabel) dalam dataset yang memiliki hubungan yang kuat dengan variable target "Global_Sales"
corr_matrix = df_combined.corr()  # Calculate the correlation matrix
corr_matrix = abs(corr_matrix["Global_Sales"])

# Selecting highly correlated features
relevant_features = corr_matrix[corr_matrix>0.4]
relevant_features

from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder

categoricalColumns = ['Platform', 'Genre', 'Publisher']
stages = []

for categoricalCol in categoricalColumns:
    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + "Index")
    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + "classVec"])
    stages += [stringIndexer, encoder]

label_stringIdx = StringIndexer(inputCol="Global_Sales", outputCol="label")
stages += [label_stringIdx]

numericCols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']
assemblerInputs = [c + "classVec" for c in categoricalColumns] + numericCols
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")
stages += [assembler]

from pyspark.ml import Pipeline

pipeline = Pipeline(stages=stages)
pipelineModel = pipeline.fit(data_num)
df = pipelineModel.transform(data_num)

selectedCols = ['label', 'features'] + [c for c in df.columns if c not in ['label', 'features']]

# The rest of your code should remain the same
df = df.select(selectedCols)
df.printSchema()

"""# Random Forest"""

train, test = df.randomSplit([0.8, 0.2], seed=2023)
print("Training Dataset Count: " + str(train.count()))
print("Test Dataset Count: " + str(test.count()))

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Random Forest Model
rf = RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=10)

# Melatih model dengan data training
rfModel = rf.fit(train)

from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve
from pyspark.sql.functions import col

# Prediksi data testing
predictions = rfModel.transform(test)  # Use transform on the test data

# Extract predicted and actual labels
y_pred = predictions.select("prediction").rdd.flatMap(lambda x: x).collect()
y_true = predictions.select("label").rdd.flatMap(lambda x: x).collect()


# Evaluasi model
accuracy = accuracy_score(y_true, y_pred)
print("Akurasi:", accuracy)

# Laporan klasifikasi
print("Report")
print(classification_report(y_true, y_pred))

"""# Gradient Boosted Decision Tree"""

!pip install graphviz

# Import Library
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import export_graphviz
import graphviz

# Lakukan one-hot encoding untuk kolom kategorikal
df_encoded = pd.get_dummies(
    raw,
    columns=['Platform', 'Genre', 'Publisher'],
    drop_first=True  # Hindari multicollinearity
)

# Pisahkan fitur (X) dan target (y)
X = df_encoded.drop(columns=['Global_Sales'])  # Pastikan X adalah DataFrame
y = df_encoded['Global_Sales']

# Split data menjadi training dan testing (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Inisialisasi model GBDT
gbdt_model = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gbdt_model.fit(X_train, y_train)

# Prediksi dan evaluasi
y_pred_gbdt = gbdt_model.predict(X_test)
threshold = 0.5
y_test_binary = (y_test > threshold).astype(int) #

from sklearn.metrics import mean_squared_error, r2_score
mse_gbdt = mean_squared_error(y_test, y_pred_gbdt)
r2_gbdt = r2_score(y_test, y_pred_gbdt)

print("\nGradient Boosted Decision Tree Metrics:")
print(f"Mean Squared Error: {mse_gbdt}")
print(f"R-squared: {r2_gbdt}")

# Visualisasi pohon pertama
tree_gbdt = gbdt_model.estimators_[0][0]

# Ekspor ke DOT dengan nama fitur
dot_data = export_graphviz(
    tree_gbdt,
    out_file=None,
    feature_names=X.columns.tolist(),
    filled=True,
    rounded=True,
    special_characters=True
)

# Render visualisasi
graph = graphviz.Source(dot_data, format='svg')
display(graph)

print("\nClassification Report:")
print(classification_report(y_test_binary, (y_pred_gbdt > threshold).astype(int)))

"""# Naive Bayes

"""

from sklearn.naive_bayes import GaussianNB
from pyspark.ml.feature import VectorAssembler
# Convert PySpark DataFrames to Pandas DataFrames
train_pd = train.toPandas()
test_pd = test.toPandas()

# Extract features and labels for training
X_train = train_pd['features'].apply(lambda x: x.toArray()).tolist() # Convert features to list of arrays
y_train = train_pd['label'].tolist()

# Extract features and labels for testing
X_test = test_pd['features'].apply(lambda x: x.toArray()).tolist() # Convert features to list of arrays
y_test = test_pd['label'].tolist()


# Training the Naive Bayes model on the Training set
# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive bayes
classifier_model_multiclass = GaussianNB()

# Memasukkan data training pada fungsi klasifikasi naive bayes
classifier_model_multiclass.fit(X_train, y_train)

# Predict Test Data
y_predict = classifier_model_multiclass.predict(X_test)
print("Prediksi Naive Bayes : ",y_predict)

#menentukan probabilitas hasil prediksi
classifier_model_multiclass.predict_proba(X_test)

#  Evaluate Model
from sklearn.metrics import confusion_matrix # Import confusion_matrix

cm = confusion_matrix(y_test, y_predict)
cm

print(classification_report(y_test,y_predict,zero_division=0))

"""# Artificial Neural Network"""

from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
# Convert PySpark DataFrames to Pandas DataFrames
train_pd = train.select('NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales').toPandas() # Select only numerical features
test_pd = test.select('NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales').toPandas() # Select only numerical features

# Normalisasi Data
scaler = StandardScaler()
X_train = scaler.fit_transform(train_pd)
X_test = scaler.transform(test_pd)

# Get labels as numpy arrays
y_train = train.select('label').toPandas()['label'].values
y_test = test.select('label').toPandas()['label'].values

# one-hot encoded labels:
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Membangun Neural Network
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=X_train.shape[1]))  # Input Layer + Hidden Layer 1
model.add(Dense(16, activation='relu'))  # Hidden Layer 2
model.add(Dense(y_train.shape[1], activation='softmax'))  # Output Layer

# Kompilasi Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Latih Model
model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1)

# Evaluasi Model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Model Accuracy: {accuracy:.2f}")

# Prediksi
predictions = model.predict(X_test)
print("Predictions:\n", predictions)

from sklearn.metrics import accuracy_score, classification_report
from pyspark.sql.functions import col
import numpy as np

# Extract predicted and actual labels
y_pred = np.argmax(predictions, axis=1)
y_true = test.select("label").toPandas()['label'].values


# Evaluasi model
accuracy = accuracy_score(y_true, y_pred)
print("Akurasi:", accuracy)

# Laporan klasifikasi
print("Report")
print(classification_report(y_true, y_pred))

from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.preprocessing import LabelBinarizer

# Menghitung probabilitas prediksi (untuk ROC dan AUC)
# Get the number of unique classes in y_true
n_classes = len(np.unique(y_true))

# Binarize the true labels for multi-class ROC AUC calculation
lb = LabelBinarizer()
y_true_binarized = lb.fit_transform(y_true)

# Ensure y_score has the correct number of columns (one for each class)
# and that the values sum up to 1.0 for each sample
y_score = predictions[:, :n_classes]  # Select the first n_classes columns

# Normalize predictions to represent probabilities
y_score = y_score / y_score.sum(axis=1, keepdims=True)

# Mengevaluasi AUC (untuk multi-class menggunakan metode "ovr")
auc_score = roc_auc_score(y_true_binarized, y_score, multi_class="ovr")
print("AUC Score (ROC):", auc_score)

# Plot ROC Curve (opsional, contoh untuk satu class)
plt.figure(figsize=(8, 6))
for i in range(y_score.shape[1]):  # Untuk setiap kelas
    fpr, tpr, _ = roc_curve(y_true_binarized[:, i], y_score[:, i])
    plt.plot(fpr, tpr, label=f'Class {i} (AUC: {roc_auc_score(y_true_binarized[:, i], y_score[:, i]):.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Garis diagonal sebagai referensi
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid()
plt.show()